%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{article}

% Можно вставить разную преамбулу
\input{preamble}

\title{\begin{center} \includegraphics[width=0.99\textwidth]{logo.png} \end{center} Метод моментов\footnote{Эта pdf-ка, по факту, представляет из себя немного переделанный конспект от Бориса Демешева:  \url{https://github.com/bdemeshev/pr201/tree/master/meth_moments}}}
\date{ } %\today}
% \author{Демешев Боря, Ульянкин Филя, Романенко Саша}

\begin{document} % Конец преамбулы, начало файла

\maketitle

\epigraph{-- Никогда не поздно поставить какую-то цель или обрести новую мечту, ведь наша жизнь длиться всего лишь момент и представляет из себя бескрайнее море возможностей. \newline -- Опять не сдал? \newline -- Опять. }{Диалог двух друзей студентов}

Пусть случайные величины $X_{1}$, ..., $X_{n}$ независимы и одинаково распределены. Закон больших чисел говорит нам, что среднее выборочное $ \bar{X} $ является хорошей оценкой для математического ожидания $ \E(X_{i}) $:

\[ \bar{X}_{n} \stackrel{P}{\longrightarrow} \E(X_{i}) \]

На практике это означает, что при больших $ n $ эти величины равны:

\[ \bar{X}_{n}\approx \E(X_{i}) \]

На этой нехитрой идее и построен метод моментов. Как конкретно используется идея, понятно из следующих двух примеров:

\begin{problem}{(киндеры)}
	Максим любит киндеры и собирает коллекцию жирОфов. Для этого он покупает шоколадки. Пусть вероятность $p$ --- вероятность того, что в киндере лежит жироф.
	
	Максим считает сколько яиц надо купить, чтобы у него появился очередной жироф. Каждый раз Макс записывает номер попытки, с которой у него появилась правильная игрушка. Обозначим эти величины $X_{1}$, ..., $ X_{n} $. Постройте оценку неизвестного параметра $ p $ с помощью метода моментов. 
\end{problem}

\begin{sol}
	Величины $ X_{i} $ имеют геометрическое распределение, поэтому $ \E(X_{i})=\frac{1}{p} $. Принцип метода моментов гласит:
	
	\[ \bar{X}_{n}\approx \frac{1}{p}\]
	Выражаем неизвестный параметр $ p $:
	\[ p\approx \frac{1}{\bar{X}_{n}} \]
	Это и есть нужная нам оценка:
	\[ \hat{p}_{MM} = \frac{1}{\bar{X}_{n}} \]
\end{sol}


\begin{problem}{(равномерное)}
Допустим, что случайные величины $ X_{1} $, ..., $ X_{n} $ независимы и равномерны на $ [\theta;\theta+1] $. Постройте оценку неизвестного параметра $ \theta $ с помощью метода моментов.
\end{problem}

\begin{sol}
В данном случае $ \E(X_{i})=\theta+0.5 $ и, следовательно:

\[ \bar{X}_{n}\approx \theta+0.5 \]
Выражаем $ \theta $:
\[ \theta\approx \bar{X}_{n}-0.5 \]
Это и есть нужная нам оценка:
\[ \hat{\theta}_{MM}=\bar{X}_{n}-0.5 \]
\end{sol}


Если говорить более формально...

\textbf{Определение.} Пусть $ X_{i} $ одинаково распределены и независимы, а $ \E(X_{i}) $ зависит от неизвестного параметра $ \theta $, скажем $ \E(X_{i})=g(\theta) $. Тогда \indef{оценкой метода моментов} называется случайная величина:

\[ \hat{\theta}_{MM} = g^{-1}(\bar{X}_{n}) \]

Конечно, иногда бывают ситуации, когда математическое ожидание $ \E(X_{i}) $ не зависит от $ \theta $. Например, если $ X_{i} $ равномерны на $ [-\theta;\theta] $, то математическое ожидание $ \E(X_{i})=0 $. Что делать в такой ситуации? 

Неспроста же наш метод называется методом моментов\ldots  Напомним, что $ k $-ым моментом случайной величины $ X_{i} $ называется математическое ожидание $ \E(X_{i}^{k}) $ \ldots 


Итак, если условия $\bar{X}_{n}\approx \E(X_{i})$ связанного с первым моментом не хватило, то на помощь придет второй момент случайной величины. В силу того же закона больших чисел:

\[ \frac{\sum X_{i}^{2}}{n} \approx \E(X_{i}^{2})\]


\begin{problem}{(равномерное)}
Величины $ X_{i} $ независимы и равномерны на $ [-\theta;\theta] $. Постройте оценку неизвестного параметра $ \theta $ с помощью метода моментов.
\end{problem}

\begin{sol}
Убеждаемся, что $\E(X_{i})=0$:

\[  \E(X_{i}) = \int _{-\theta}^{\theta} x \cdot \frac{1}{2\theta} \dx{x} = \left. \frac{x^2}{4 \theta} \right|_{-\theta}^{\theta} = \left( \frac{\theta^2 - (-\theta)^2 }{4 \theta}  \right) =  \frac{\theta^{2} - \theta^{2}}{4 \theta} = 0  \]

Находим $ \E(X_{i}^{2}) $:

\[  \E(X_{i}^{2}) = \int _{-\theta}^{\theta} x^{2} \cdot \frac{1}{2\theta}  \dx{x} = \left. \frac{x^3}{6 \theta} \right|_{-\theta}^{\theta} = \left( \frac{\theta^3 - (-\theta)^3 }{6\theta}  \right) = \frac{2 \theta^{3}}{6\theta}  = \frac{\theta^{2}}{3}  \]

Согласно принципу метода моментов:

\[ \frac{\sum X_{i}^{2}}{n} \approx \frac{\theta^{2}}{3} \]

Выражаем $ \theta $:

\[ \theta\approx \sqrt{3\frac{\sum X_{i}^{2}}{n} }\]

Это и есть нужная нам оценка:

\[ \hat{\theta}_{MM}= \sqrt{3\frac{\sum X_{i}^{2}}{n} } = \sqrt{3 \overline{X^2 } }\]

Если не хватит и второго момента, тогда воспользуемся третьим и т.д. Для произвольного $k$ мы имеем:

\[ \frac{\sum X_{i}^{k}}{n} \approx \E(X_{i}^{k})\]

В большинстве случаев хватает именного первого момента. Последующие моменты нужны чаще всего при оценке нескольких параметров.
\end{sol}

\end{document}