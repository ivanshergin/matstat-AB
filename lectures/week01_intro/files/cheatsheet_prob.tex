%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{article}

% Можно вставить разную преамбулу
\input{preamble}

\title{\begin{center} \includegraphics[width=0.99\textwidth]{logo.png}\end{center}Теоретический минимум по теории вероятностей\footnote{Это краткая выжимка с основными определениями из теории вероятностей. Она не претендует на полноту. Частично шпаргалка основана на материале \url{https://github.com/bdemeshev/pr201/tree/master/cheat_sheet}}}
\date{ } %\today}
% \author{Ульянкин Филя, Романенко Саша}

\begin{document} % Конец преамбулы, начало файла

\maketitle

\section{Условная вероятность случайного события}

Условной вероятностью события $A$ при условии, что произошло событие $B$, называется число:

$$
\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}.
$$

Из формулы условной вероятности можно получить формулу для вероятности произведения нескольких событий: 

$$
\PP(A \cap B) = \PP(A \mid B) \cdot \PP(B).
$$

Если событий несколько, формулу можно продолжить:

$$
\PP(A \cap B \cap C) = \PP(A \mid B, C) \cdot \PP(B \mid C) \cdot \PP(C).
$$

\newpage

\section{Независимость событий}

Говорят, что два события попарно \indef{независимы,} если верно следующее:

$$
\PP(A \cap B) = \PP(A) \cdot \PP(B)
$$

Говорят, что $n$ случайных событий \indef{независимы в совокупности,} если для любого $1 \leq k \leq n$ и любого набора различных меж собой индексов $1 \leq i_1, ..., i_k \leq n$ имеет место равенство:

$$
\PP(A_{i_1} \cap ... \cap A_{i_k}) = \PP(A_{i_1}) \cdot ... \cdot \PP(A_{i_k})
$$


\section{Формула полной вероятности}

Пусть событие $A$ происходит вместе с одним из событий $H_1, H_2, \ldots, H_k.$ Пусть эти события попарно несовместны (ещё говорят, что они составляют  \indef{полную группу.)}

Нам известны вероятности этих событий $\PP(H_1), \PP(H_2), \ldots, \PP(H_k)$, а также условные вероятности события $A$:  $\PP(A \mid H_1), \PP(A \mid H_2), \ldots, \PP(A \mid H_k)$.

Тогда вероятность события $A$ может быть вычислена по формуле:

$$
\PP(A) = \sum_{i=1}^{\infty} \PP(H_i) \cdot \PP(A \mid H_i).
$$


\section{Формула Байеса}

Пусть событие $A$ происходит вместе с одним из событий $H_1, H_2, \ldots, H_k,$ которые составляют полную группу и попарно несовместны. 

Пусть известно, что в результате испытания событие $A$ произошло. Тогда условная вероятность того, что имело место событие $H_k$, можно пересчитать по формуле:

$$
\PP(H_k \mid A) = \dfrac{\PP(H_k \cap A)}{P(A)} = \dfrac{\PP(H_k) \cdot \PP(A \mid H_k)}{\sum_{i=1}^{\infty} \PP(H_i) \cdot \PP(A \mid H_i)}
$$

\newpage

\section{Функция распределения случайной величины}

\indef{Функцией распределения} случайной величины $X$ называется функция $F_X(x),$ определённая для любого действительного числа $x \in \mathbb{R}$ и выражающая собой  вероятность того, что случайная величина $X$ примет значение, лежащее на числовой прямой левее точки $x$, то есть

$$
F_X(x) = \PP(X \leq  x).
$$

Любая функция распределения обладает следующими свойствами:

\begin{itemize}
\item Принимает значения в диапазоне от $0$ до $1$, при этом:  

$$
\lim_{x \to +\infty} F_X(x)=1 \qquad \lim_{x \to -\infty} F_X(x) = 0
$$

\item $F_X(x)$ не убывает: $F_X(x_1) \leq F_X(x_2) \quad \forall x_1 \leq x_2$

\item $F_X(x)$ непрерывна справа: $\lim\limits_{x \to x_0^+} F_X(x) = F_X(x_0)$
\end{itemize}

\section{Функция плотности случайной величины и ее свойства}

Случайная величина $X$ имеет \indef{абсолютно непрерывное распределение,} если существует неотрицательная функция $f_X(x)$ такая, что 

$$ 
F_X(x) =\int_{-\infty}^{x} f_X(t) \dx{t}.
$$

Функция $f_X(x)$ называется \indef{функцией плотности распределения} случайной величины $X$.

Свойства функции плотности:

\begin{itemize}
\item Неотрицательно определена: $ f_X(x) \geq 0$

\item Площадь под плотностью распределения всегда равна единице: $$\int_{-\infty}^{+\infty} f_X(t)  \dx{t} = 1$$

\item С помощью плотности можно найти вероятность того, что случайная величина попадёт в конкретный отрезок: 

$$
\PP(a \le X \le b) = \int_a^b f_X(x) \dx{x} = F_X(b) - F_X(a)
$$
\end{itemize}


\section{Функция совместного распределения двух случайных величин}

Пусть у нас ест две случайные величины $X$ и $Y$.  \indef{Совместной функцией распределения} двумерной случайной величины будет называться функция, определённая  $ \forall x,y \in \RR$ и выражающая собой вероятность одновременного выполнения событий $X \leq x,$ и $Y \leq y$:

$$
F(x,y) = P(X \leq x, Y \leq y)
$$

Свойства функции распределения аналогичны одномерному случаю: 

\begin{itemize}
	\item Принимает значения в диапазоне от $0$ до $1$, при этом:  
	
	$$
	F(x, -\infty) = F(-\infty, y) = F(-\infty, -\infty) = 0, \qquad  F(+\infty,+\infty) = 1
	$$
	
	\item $F_X(x)$ не убывает по каждому из своих аргументов
	
	\item $F_X(x)$ непрерывна справа по каждому из своих аргументов 
	
	\item Если один из аргументов стремится к бесконечности, то получится функция распределения другой составляющей: $$ F(x,+\infty)  = \PP(X< x, Y < +\infty) = \PP(X< x) = F_X(x) $$
\end{itemize}

Случайные величины $X$ и $Y$ \indef{независимы,} если их функция распределения равна произведению функций распределения составляющих:

$$
F(x,y) = F_{X}(x) \cdot F_{Y}(y)  
$$

Для $n$ случайных величин функцию распределения можно задать по аналогии. 

\section{Совместная плотность распределения двух случайных величин}

Случайные величины $X$, $Y$ имеют \indef{абсолютно непрерывное совместное распределение,} если существует функция $f(x, y) \ge 0$ такая, что $\forall x,y$ совместная функция распределения представима в виде: 

$$
F(x,y) = \int_{-\infty}^x \int_{-\infty}^y f(t_1, t_2) \dx{t_1}\dx{t_2}
$$

Если такая функция $f(x, y)$ существует, она называется плотностью совместного распределения случайных величин $X$ и $Y$.

Свойства совместной функции плотности:

\begin{itemize}
\item  $f(x,y) \ge 0$
\item  Площадь под совместной плотностью распределения равна единице: 
$$
\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}  f(t_1, t_2) \dx{t_1}\dx{t_2} = 1
$$
\item Чтобы получить плотность распределения одной из составляющих, можно выинтегрировать из совместной плотности все значения другой:

\begin{equation*}
\begin{aligned}
& f_X(x) = \int_{-\infty}^{+\infty} f(x,y) \dx{y} \\ 
& f_Y(y) = \int_{-\infty}^{+\infty} f(x,y) \dx{x} \\ 
\end{aligned}
\end{equation*}
\end{itemize}

Случайные величины $X$ и $Y$ с абсолютно непрерывными распределениями \indef{независимы,} если плотность их совместного распределения существует и равна
произведению частных функций плотности:

$$
f(x,y) = f_X(x) \cdot f_Y(y)
$$

%%------------------------------------------------------
\section{Математическое ожидание}

\textbf{Интуитивно:} среднее арифметическое значение величины при многократном повторении случайного эксперимента

\indef{Математическим ожиданием} $\E(X)$ непрерывной случайной величины $X$ называется число:

$$
\E(X) = \int_{-\infty}^{+\infty} t \cdot f_X(t)  \dx{t}, 
$$ 

\indef{Математическим ожиданием} $\E(X)$ дискретной случайной величины $X$  называется число:

$$
\E(X) = \sum\limits_{k} a_k \cdot \PP(X = a_k),
$$ 

\textbf{Свойства:}

\begin{itemize}
\item $\E(a \cdot  X + b \cdot Y + c) = a \cdot \E(X) + b \cdot E(Y) + c$
\item Если $X \geq Y$ почти наверное, то $\E(X) \geq \E(Y)$
\item Если $X$ и $Y$ независимы и их математические ожидания существуют, то $$\E(X \cdot Y) = \E(X) \cdot \E(Y)$$
\end{itemize}


\section{Дисперсия}

\textbf{Интуитивно:} мера разброса случайной величины. \textbf{Геометрический смысл:} квадрат длины случайной величины.

\indef{Дисперсия} случайной величины $\Var(X)$ --- это число 

$$
\Var(X) = \E(X - \E(X))^2 =  E(X^2) - E^2(X)
$$

Дисперсия --- это среднее значение квадрата отклонения случайной величины $X$ от своего среднего.

\textbf{Свойства:}

\begin{itemize}
\item $\Var(X) \ge 0$
\item $\Var(X) = 0$ равносильно тому, что $\PP(X=\const)=1$
\item $\Var(a \cdot X + b) = a^2 \cdot \Var(X)$
\item $\Var(X + Y) = \Var(X) + \Var(Y) + 2\cdot \Cov(X,Y)$
\item $\Var(X + Y) = \Var(X) + \Var(Y)$, если величины линейно независимы
\end{itemize}

\section{Стандартное отклонение}

\indef{Стандартным отклонением} называют корень из дисперсии:

$$
\sigma(X)=\sqrt{\Var(X)}
$$

Эту величину вводят, так как дисперсия измеряется в квадратах (лет$^2$, м$^2$) и т.п.

\textbf{Свойства:}

\begin{itemize}
\item $\sigma(X) \ge 0$
\item $\sigma(X) = 0$ равносильно тому, что $\PP(X=\const)=1$
\item $\sigma(a \cdot X + b) = |a| \cdot \sigma(X)$
\end{itemize}


\section{Ковариация}

\textbf{Интуитивно:} мера линейной связи величин $X$ и $Y$

\indef{Ковариацией} между двумя случайными величинами называют величину

$$
\Cov(X, Y) = \E\left( [X - \E(X)] \cdot [Y - \E(Y)] \right) = \E(X \cdot Y) - \E(X) \cdot E(Y)
$$

\textbf{Геометрический смысл:} скалярное произведение случайных величин

\textbf{Свойства:}

\begin{itemize} 
\item Если $X$ и $Y$ независимы, то их ковариация равна нулю, но обратное неверно. Нулевая ковариация означает отсутствие линейной взаимосвязи. Взаимосвязь может быть устроена сложнее.
\item $\Cov(X, Y) = \Cov(Y, X)$
\item $\Cov(a \cdot X + b, Y) = a \cdot \Cov(X, Y)$
\item $\Cov(X + Y, Z) = \Cov(X, Z) + \Cov(Y, Z)$
\end{itemize}


\section{Корреляция}

\textbf{Интуитивно:} отнормированная мера линейной связи величин $X$ и $Y$

\indef{Коэффициентом корреляции} $\Corr(X, Y)$ случайных величин $X$ и $Y$ называется число:

$$
\Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X)} \cdot \sqrt{\Var(Y)}}.
$$

\textbf{Геометрический смысл:} косинус угла между случайными величинами

\textbf{Свойства:}

\begin{itemize} 
\item $ -1 \leq \Corr(X, Y) \leq 1$
\item  $\left| \Corr(X, Y) \right| = 1 \Leftrightarrow \exists a, b \in \RR: X = a \cdot Y + b$
\item $\Corr(a \cdot X + b, c \cdot Y + d) = \sgn(ac) \cdot \Corr(X, Y)$
\item $Corr(X,Y) = Corr(Y, X)$
\item $\Corr(X,Y)=0$ означает отсутствие линейной зависимости между $X$ и $Y$, но зависимость может быть устроена сложнее.
\end{itemize} 


\section{Закон больших чисел в слабой форме}

Пусть $X_1, \ldots, X_n$ попарно независимые и одинаково распределённые случайные величины с конечным вторым моментом, $\E(X_i^2) < \infty$, тогда имеет место сходимость:

$$
\frac{X_1 + \ldots + X_n}{n} \overset{p}{\to} \frac{\E(X_1) + \E(X_2) + \ldots + \E(X_n)}{n}
$$

Если у случайных величин одинаковое математическое ожидание, тогда: 

$$
\frac{X_1 + \ldots + X_n}{n} \overset{p}{\to} \E(X_1)
$$

\section{Центральная предельная теорема}

Пусть $X_1, \ldots, X_n$ случайные величины, имеющие одинаковое распределение с конечными математическим ожиданием и дисперсией:

$$
X_1, \ldots, X_n \sim iid(\mu,\sigma^2).
$$

тогда при $n \to \infty$ имеет место сходимость по распределению: 

$$
\frac{X_1 + \ldots +  X_n - \mu \cdot n}{\sqrt{n} \sigma } \overset{d}{\to} N(0,1)
$$

\section{Сходимость по вероятности}

Говорят, что последовательность случайных величин $X_1, X_2, \ldots$ сходится к случайной величине $X$ при $n \to \infty$ \indef{по вероятности,} и пишут $X_n \overset{p}{\to} X$, если для любого  $\varepsilon > 0$:

$$
\PP(|X_n - X| \ge \varepsilon) \to 0
$$

\section{Сходимость по распределению}

Говорят, что последовательность случайных величин $X_1, X_2, \ldots$ сходится к случайной величине $X$ при $n \to \infty$ \indef{по распределению,} и пишут $X_n \overset{d}{\to} X$, если $F_{X_n}(x) \to F_X(x)$ для всех $x$, в которых $F_X(x)$ непрерывна.


\section{Основные распределения}

\subsection*{Биномиальное распределение}

\indef{Биномиальное распределение} --- дискретное распределение количества успехов среди $n$ испытаний с вероятностью успеха, равной $p$. Обычно записывают как:

$$
X \sim Binom(n, p)
$$

Вероятность того, что произойдёт $k$ успехов расчитывается по формуле: 

$$
\PP(X = k) = {C}^n_k \cdot p^k \cdot (1 - p)^{n - k}, \quad k \in \{0, 1, \ldots, n\}
$$

\textbf{Пример, когда возникает:} сколько раз человек попадёт в баскетбольную корзину при $n$ бросках 

\textbf{Свойства:}

\begin{itemize} 
\item $\E(X) = n \cdot p$
\item $\Var(X) = n \cdot p \cdot (1 - p)$
\end{itemize} 


\subsection*{Распределение Пуассона}

\indef{Распределение Пуассона} --- распределение дискретной случайной величины, представляющей собой число событий, произошедших за фиксированное время, при условии, что данные события происходят с некоторой фиксированной средней интенсивностью $\lambda$ и независимо друг от друга. Хорошо подходит для моделирования счётчиков. Обычно записывают как:

$$
X \sim \Pois(\lambda)
$$

Вероятность того, что произойдёт $k$ событий расчитывается по формуле: 

$$
\PP(X = k) = e^{-\lambda} \dfrac{\lambda^k}{k!}, \quad k \in \{0, 1, \ldots, \}
$$

\textbf{Пример, когда возникает:} число лайков под фотографией, любая случайная величина счётчик, которая подчиняется аксиомам простейшего потока событий

\textbf{Свойства:}

\begin{itemize} 
\item $\E(X) = \lambda$
\item $\Var(X) = \lambda$
\end{itemize} 

\subsection*{Геометрическое распределение}

\indef{Распределение Пуассона} --- распределение дискретной случайной величины, представляющей собой номер первого успеха в серии испытаний Бернулли. Обычно записывают как:

$$
X \sim \Geom(p)
$$

Вероятность того, что номер первого успеха равен $k$ находится как:

$$
\PP(X = k) = p \cdot (1 - p)^{k - 1}
$$

\textbf{Пример, когда возникает:} номер попытки, при которой игрок попал в баскетбольную корзину

\textbf{Свойства:}

\begin{itemize} 
\item $\E(X) = \frac{1}{p}$
\item $\Var(X) = \frac{1-p}{p^2}$
\end{itemize} 


\subsection*{Равномерное распределение}

\indef{Равномерное распределение на отрезке $[a;b]$} обладает плотностью распределения: 

$$
f_X(x) =\begin{cases}
\frac{1}{b - a}, \quad x \in [a; b]  \\
0, \quad x \notin [a; b]
\end{cases}
$$

Обычно записывают как:

$$
X \sim \mU[a; b]
$$

\textbf{Пример, когда возникает:} остаток при округлении чисел

\textbf{Свойства:}

\begin{itemize} 
\item $\E(X) = \frac{a + b}{2}$
\item $\Var(X) = \frac{(b - a)^2}{12}$
\end{itemize} 


\subsection*{Экспоненциальное распределение}

\indef{Экспоненциальное распределение} обладает плотностью распределения: 

$$
f_X(x) =\begin{cases}
\lambda e^{- \lambda x}, \quad x \ge 0  \\
0, \quad x < 0
\end{cases}
$$

Обычно записывают как:

$$
X \sim \Exp(\lambda)
$$

\textbf{Пример, когда возникает:} время между событиями, имеющими распределение Пуассона (время, пока следующий человек придёт в кассу, время до следующего лайка под фото и тп)

\textbf{Свойства:}

\begin{itemize} 
\item $\E(X) = \frac{1}{\lambda}$
\item $\Var(X) = \frac{1}{\lambda^2}$
\end{itemize} 

\subsection*{Нормальное распределение}

Говорят, что у случайной величины $X$ \indef{нормальное распределение с параметрами $\mu$ и $\sigma^2$}, если она обладает плотностью распределения

$$
f_{X}(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\tfrac{(x - \mu)^2}{2\sigma^2}}
$$

Обычно записывают как:

$$
X \sim \mN(\mu, \sigma^2)
$$

\textbf{Пример, когда возникает:} нахождение суммы или среднего большого количества независимых одинаково распределенных величин

\textbf{Свойства:}

\begin{itemize} 
\item $\E(X) = \mu$
\item $\Var(X) = \sigma^2$
\item Если $X \sim \mN(\mu_1, \sigma_1^2)$ и $Y \sim \mN(\mu_2, \sigma_2^2)$, тогда 

$$
a\cdot X + b \cdot Y + c \sim \mN(a\cdot \mu_1 + b \cdot \mu_2 + c, a^2 \sigma_1^2 + b^2 \sigma_2^2) 
$$

\item Для нормального распределения выполняются правила одной, двух и трёх сигм: 

\begin{equation*}
\begin{aligned}
& \PP(\mu - \sigma \le X \le \mu + \sigma) \approx = 0.683 \\
& \PP(\mu - 2\cdot \sigma \le X \le \mu + 2 \cdot \sigma) \approx = 0.954 \\
& \PP(\mu - 3 \cdot \sigma \le X \le \mu + 3 \cdot \sigma) \approx = 0.997
\end{aligned}
\end{equation*}
\end{itemize} 


\subsection*{"Хи-квадрат" распределение}

Пусть случайные величины $X_1, \ldots, X_k$ независимы и одинаково распределены. Причём нормально с параметрами $0$ и $1$. Обычно такой факт записывают следующим образом: 

$$
X_1, \ldots, X_k \sim iid \hspace{2mm} N(0,1).
$$ 

Буквы $iid$ расшифровываются как identically independently distributed (независимы и одинаково распределены).

Случайная величина $Y = X_1^2 + \ldots X_k^2$ имеет \indef{распределение хи-квадрат с $k$ степенями свободы.}  Степень свободы это просто название для параметра распределения.

Обычно записывают как:

$$
Y \sim \chi^2_k
$$   

\textbf{Пример, когда возникает:} на практике тесно связано с выборочной дисперсией для нормальных выборок

\textbf{Свойства:}

\begin{itemize} 
\item $\E(\chi^2_k) = k \cdot \E(X_i^2) = k$
\item $\Var(\chi^2_k) = k \cdot \E(X_i^4) = 2k$
\item Распределение устойчиво к суммированию. То есть, если $\chi^2_k$ и $\chi^2_m$ независимы, тогда $\chi^2_k + \chi^2_m$ = $\chi^2_{k+m}$
\item $\frac{\chi^2_k}{k} \to 1$ по вероятности. 
\end{itemize} 


\subsection*{Распределение Стьюдента}

Пусть случайные величины

$$
X_0, X_1, \ldots, X_k \sim iid \hspace{2mm} N(0,1),
$$ 

тогда случайная величина $$ Y = \frac{X_0}{\sqrt{^{\chi^2_k}/_k}} $$ имеет \indef{распределение Cтьюдента с $k$ степенями свободы.}  
Обычно записывают как:

$$
Y \sim t(k)
$$   

\textbf{Пример, когда возникает:} на практике тесно связано с отношением выборочного среднего и стандартного отклонения нормальных выборок

\textbf{Свойства:}

\begin{itemize} 
\item $\E(Y) = 0, k > 1$
\item $\Var(Y) = \frac{k}{k-2}, k > 2$
\item Симметрично относительно нуля
\item $t(k) \to N(0,1)$ по распределению при $k \to \infty$
\item При $k=1$ совпадает с распределением Коши
\end{itemize} 


\subsection*{Распределение Фишера}

Говорят, что случайная величина 

$$ Y = \frac{^{\chi^2_k}/_k}{^{\chi^2_m}/_m}$$

имеет \indef{распределение Фишера c $k,m$ степенями свободы}.

Обычно записывают как:

$$
Y \sim F_{k,m}
$$   

\textbf{Пример, когда возникает:} на практике тесно связано с отношением выборочных дисперсий двух нормальных выборок

\textbf{Свойства:}

\begin{itemize} 
\item $\E(Y) = \frac{m}{m-2}, m > 2$
\item $\Var(Y) = \frac{2m^2(m + k - 2)}{k (m - 2)^2 (m - 4) }, m > 4$
\item Если $Y \sim F(k,m)$, тогда $\frac{1}{Y} \sim F(m,k)$
\item При $k \to \infty$ и $m \to \infty$ $F(k,m) \to 1$ по вероятности
\item А вот этот факт не раз всплывёт в эконометрике: $t_k^2 = F(1,k)$
\end{itemize} 

\section*{Квантильное преобразование}

\textbf{Теорема:}

Пусть функция распределения $F_X(x)$ непрерывна. Тогда случайная величина $Y = F(X)$ имеет равномерное распределение на отрезке $[0; 1]$.

\textbf{Следствие:}

Пусть $Y \sim U[0;1]$, а $F(x)$ произвольная функция распределения. Тогда случайная величина $X = F^{-1}(Y)$ будет иметь функцию распределения $F(x)$.

\end{document}