%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{article}

% Можно вставить разную преамбулу
\input{preamble}

\title{\begin{center} \includegraphics[width=0.99\textwidth]{logo.png} \end{center}  О карасях, рыбалке и бабушках\footnote{Эта pdf-ка, по факту, представляет из себя кусочек недописанной виньетки по Байесовским методам: \newline  \url{https://github.com/FUlyankin/book_about_bayes}}}
\date{ } %\today}
% \author{Ульянкин Филя, Романенко Саша}

\begin{document}

\maketitle

\epigraph{When the facts change, I change my mind. What do you do, sir?}{John Maynard Keynes}

Рассмотрим простой пример. Пусть в озере живут караси и щуки. Петя, живущий в деревне по соседству, выловил в нём карася, щуку и ещё одного карася, а после серьёзно задумался о том с какой вероятностью, $p$, он таскает карасей из озера. Петя предполагает, что в озере настолько много рыбы, что вылов одного карася несильно меняет вероятность поймать нового карася, т.е. наблюдения $y_1 = 1, y_2 = 0, y_3 = 1$ независимы и одинаково распределены.

\section*{О том какие у бабушек бывают распределения}

Если бы Петя был частотным статистиком, то он бы воспользовался методом максимального правдоподобия или методом наименьших квадратов и нашёл бы оценку требуемой вероятности. Тем не менее, в родной деревне Пети широко практикуется байесовское воспитание, в связи с чем ему не хотелось бы пользоваться стандартными методами.

Идея! Мы ничего не знаем о параметре $p$. Давайте опишем наше незнание с помощью какой-то \indef{априорной функции распределения.} Важно  помнить, что на данные при этом смотреть нельзя. Наши априорные ожидания никак не должны быть с ними связаны. В случае Пети, он сначала должен задать распределение $p$, а уже после идти таскать рыб.

Например, если мы вообще ничего не знаем о том, что происходит в пруду, то логично взять в качестве априорного распределения равномерное, $p \sim \mU[0;1]$. Тем самым мы не только скажем, что настолько ничего не знаем о параметре $p$, что допускаем абсолютно любое значение этого параметра, но и одновременно с этим откинем все невозможные значения, ограничив $p$ отрезком от $0$ до $1$.

\begin{figure}[H]
	\begin{minipage}[H]{0.39\linewidth}
		\begin{center}
			\[ f(p) = \begin{cases}
			1&, p \in [0;1] \\
			0&, \text{иначе}\\
			\end{cases} \]
		\end{center}
	\end{minipage}
	\hfill
	\begin{minipage}[H]{0.59\linewidth}
		\begin{center}
			\begin{tikzpicture}
			% оси
			\draw [->] (-3.8,0) -- (4,0);
			\draw [->] (0,0) -- (0,3.5);
			% график
			\draw [blue, thick, domain=0:2] plot (\x, 2);
			\draw [->, blue, thick] (-3.8,0)--(-0.05,0);
			\draw [<-, blue, thick] (2.05,0)--(4,0);
			\draw [blue, thick,dashed] (2,0)--(2,2);
			% точки
			\draw[fill,blue] (2,2) circle [radius=0.03];
			\draw[fill,blue] (0,2) circle [radius=0.03];
			% подписи
			\node [below] at (0,0) {0};
			\node [below] at (2,0) {1};
			\node [left] at (0,2) {1};
			\node [below right] at (4,0) {$p$};
			\node [left] at (0,3.3) {$f(p)$};
			\end{tikzpicture}
		\end{center}
	\end{minipage}
\end{figure}

В то же самое время, если у нас есть любящая порыбачить (а заодно и внука) бабушка, которая говорит, что за свою жизнь выловила из озера карасей в несколько раз больше, чем щук, то вполне логично поверить ей и предположить, что у параметра $p$ будет распределение с плотностью

\begin{figure}[H]
	\begin{minipage}[H]{0.39\linewidth}
		\begin{center}
			\[ f(p) = \begin{cases}
			2p&, p \in [0;1] \\
			0&, \text{иначе.}\\
			\end{cases} \]
		\end{center}
	\end{minipage}
	\hfill
	\begin{minipage}[H]{0.59\linewidth}
		\begin{center}
			\begin{tikzpicture}[scale=0.8]
			% оси
			\draw [->] (-3.8,0) -- (4,0);
			\draw [->] (0,0) -- (0,4.5);
			% график
			\draw [blue, thick, domain=0:2] plot (\x,2*\x);
			\draw [->, blue, thick] (-3.8,0)--(-0.05,0);
			\draw [<-, blue, thick] (2.05,0)--(4,0);
			\draw [blue, thick,dashed] (2,0)--(2,4);
			\draw [blue, thick,dashed] (0,4)--(2,4);
			% точки
			\draw[fill,blue] (2,4) circle [radius=0.03];
			\draw[fill,blue] (0,4) circle [radius=0.03];
			% подписи
			\node [below] at (0,0) {0};
			\node [below] at (2,0) {1};
			\node [left] at (0,4) {2};
			\node [below right] at (4,0) {$p$};
			\node [above left] at (0,4.3) {$f(p)$};
			\end{tikzpicture}
		\end{center}
	\end{minipage}
\end{figure}

Тогда в своих априорных предположениях мы учтём многолетний опыт бабушки, а вместе с ним большое число случайных выборок из местного прудика, которые мы не видели. Если бабушка не врёт, и в пруду ничего с тех пор не поменялось, дополнительная информация поможет нам получить более точные оценки. Однако, если бабушка Пети никогда не ловила рыбу (или это вообще не его бабушка, хотя она и утверждает обратное), то принимать её априорное мнение о рыбе на веру ни в коем случае нельзя. Вы должны верить в априорное распределение и должны быть готовы сделать на него денежную ставку.

Давайте посмотрим, что у нас получится при разных априорных мнениях. Пусть $p \sim \mU[0;1]$. Найдём апостериорную плотность распределения параметра $p$. Воспользуемся формулой Байеса:

\[ f(p \mid y_1,y_2,y_3) = \frac{f(p,y_1,y_2,y_3)}{f(y_1,y_2,y_3)} = \frac{f(y_1,y_2,y_3 \mid p) \cdot f(p)}{f(y_1,y_2,y_3)}.\]

В знаменателе полученной дроби стоит значение совместной плотности распределения трёх случайных величин в точке $y_1, y_2, y_3$. Это какая-то константа. Пренебрежём ей для лёгкости расчётов. Чуть позже мы восстановим её назад. С помощью значка $\propto$ будем записывать равенство с точностью до константы

\[  \frac{f(y_1,y_2,y_3 \mid p) \cdot f(p)}{f(y_1,y_2,y_3)} \propto f(y_1,y_2,y_3 \mid p) \cdot f(p).\]

Вспоминаем о том, что собранные нами наблюдения независимы и получаем

\begin{multline*}
f(y_1,y_2,y_3 \mid p) \cdot f(p) =  f(y_1 \mid p) \cdot f(y_2 \mid p) \cdot f(y_3 \mid p) \cdot f(p) = \\ = \PP(y_1 = 1 \mid p) \cdot \PP(y_2 = 0 \mid p) \cdot \PP(y_3 = 1 \mid p) \cdot f(p) = p \cdot (1 - p) \cdot p \cdot 1.
\end{multline*}

Выходит, что апостериорная плотность распределения параметра $p$ должна иметь вид

\[ f(p \mid y_1, y_2, y_3) = const \cdot p^2 \cdot (1-p).\]

Осталось восстановить нормировочную константу. Вспоминаем, что интеграл по области определения апостериорной плотности распределения должен быть равен единице


\[ const \cdot \int_0^1 p^2 \cdot (1-p) \dx{p}  = 1 \quad \Rightarrow \quad const = 12 \]

Итак, ваши авации! Апостериорное распределение параметра $p$:

\[ f(p \mid y_1, y_2, y_3) =
\begin{cases}
12 \cdot p^2 \cdot (1-p)   & \quad p \in [0;1] \\
0      & \quad \text{иначе}\\
\end{cases}
\]

\begin{figure}[H]
	\begin{minipage}[H]{0.49\linewidth}
		\center Априорное распределение: \\[2.5ex]
		\begin{tikzpicture}
		% оси
		\draw [->] (-1.8,0) -- (4,0);
		\draw [->] (0,0) -- (0,3.5);
		% график
		\draw [blue, thick, domain=0:2] plot (\x, 2);
		\draw [->, blue, thick] (-1.8,0)--(-0.05,0);
		\draw [<-, blue, thick] (2.05,0)--(4,0);
		\draw [blue, thick,dashed] (2,0)--(2,2);
		% точки
		\draw[fill,blue] (2,2) circle [radius=0.03];f
		\draw[fill,blue] (0,2) circle [radius=0.03];
		% подписи
		\node [below] at (0,0) {0};
		\node [below] at (2,0) {1};
		\node [left] at (0,2) {1};
		\node [below right] at (4,0) {$p$};
		\node [left] at (0,3.3) {$f(p)$};
		\end{tikzpicture}
	\end{minipage}
	\hfill
	\begin{minipage}[H]{0.49\linewidth}
		\center Апостериорное распределение: \\[2.5ex]
		\begin{tikzpicture}
		% оси
		\draw [->] (-1.8,0) -- (3,0);
		\draw [->] (0,0) -- (0,3.5);
		% график12
		\draw [blue, thick, domain=0:1] plot (\x, {12*\x*\x*(1-\x)});
		\draw [blue, thick] (-1.8,0)--(0,0);
		\draw [blue, thick] (1,0)--(3,0);
		%\draw [blue, thick,dashed] (2,0)--(2,2);
		% точки
		%\draw[fill,blue] (2,2) circle [radius=0.03];
		%\draw[fill,blue] (0,2) circle [radius=0.03];
		% подписи
		\node [below] at (0,0) {0};
		\node [below] at (1,0) {1};
		% \node [left] at (0,1) {1};
		\node [below right] at (3,0) {$p$};
		\node [left] at (0,3.3) {$f(p)$};
		\end{tikzpicture}
	\end{minipage}
\end{figure}

В априорном мнении Петя не знал где находится $p$ и все точки для него были одинаково предпочтительны. Апостериорное мнение говорит, что вероятность поймать карася гораздо ближе к единице, чем к нулю. Проделаем те же самые рассуждения, но уже учитывая априорное мнение бабушки.

По аналогии получаем

\[ f(p \mid y_1, y_2, y_3) = const \cdot p^2 \cdot (1-p) \cdot 2p = const \cdot p^3 \cdot (1-p).\]

Восстанавливаем константу:

\[ const \cdot \int_0^1 p^3 \cdot (1-p) \dx{p}  = 1 \quad \Rightarrow \quad const = 20.\]

Снова получаем апостерирную функцию плотности

\[ f(p \mid y_1, y_2, y_3) =
\begin{cases}
20 \cdot p^3 \cdot (1-p)   & \quad p \in [0;1] \\
0      & \quad \text{иначе.}\\
\end{cases}
\]

\begin{figure}[H]
	\begin{minipage}[H]{0.49\linewidth}
		\center Априорное распределение: \\[2.5ex]
		\begin{tikzpicture}[scale=0.8]
		% оси
		\draw [->] (-1.8,0) -- (4,0);
		\draw [->] (0,0) -- (0,4.5);
		% график
		\draw [blue, thick, domain=0:2] plot (\x,2*\x);
		\draw [->, blue, thick] (-1.8,0)--(-0.05,0);
		\draw [<-, blue, thick] (2.05,0)--(4,0);
		\draw [blue, thick,dashed] (2,0)--(2,4);
		\draw [blue, thick,dashed] (0,4)--(2,4);
		% точки
		\draw[fill,blue] (2,4) circle [radius=0.03];
		\draw[fill,blue] (0,4) circle [radius=0.03];
		% подписи
		\node [below] at (0,0) {0};
		\node [below] at (2,0) {1};
		\node [left] at (0,4) {2};
		\node [below right] at (4,0) {$p$};
		\node [above left] at (0,4.3) {$f(p)$};
		\end{tikzpicture}
	\end{minipage}
	\hfill
	\begin{minipage}[H]{0.49\linewidth}
		\center Апостериорное распределение: \\[2.5ex]
		\begin{tikzpicture}
		% оси
		\draw [->] (-1.8,0) -- (3,0);
		\draw [->] (0,0) -- (0,3.5);
		% график12
		\draw [blue, thick, domain=0:1] plot (\x, {20*\x*\x*\x*(1-\x)});
		\draw [blue, thick] (-1.8,0)--(0,0);
		\draw [blue, thick] (1,0)--(3,0);
		%\draw [blue, thick,dashed] (2,0)--(2,2);
		% точки
		%\draw[fill,blue] (2,2) circle [radius=0.03];
		%\draw[fill,blue] (0,2) circle [radius=0.03];
		% подписи
		\node [below] at (0,0) {0};
		\node [below] at (1,0) {1};
		% \node [left] at (0,1) {1};
		\node [below right] at (3,0) {$p$};
		\node [left] at (0,3.3) {$f(p)$};
		\end{tikzpicture}
	\end{minipage}
\end{figure}

Если учесть и мнение бабушки и нашу выборку, то получится, что шансы того, что карасей в озере мало, минимальны.

Сравним между собой априорную вероятность $\PP( p > 0.5)$ и апостериорную вероятность того, что $\PP( p > 0.5 \mid y_1, \ldots, y_3)$, а также априорное и апостериорное математические ожидания, $\E(p)$ и $\E(p \mid y_1, \ldots, y_3)$.


\begin{table}[H]
	\begin{tabularx}{\textwidth}{@{}XX@{}}
		\toprule
		Равномерное распределение: & Распределение бабушки: \\
		\midrule
		$ \PP( p > 0.5 ) = \int_{0.5}^1 1 \dx{p} = 0.5 $  &   $ \PP( p > 0.5 ) = \int_{0.5}^1 2p \dx{p} = 0.75 $ \\
		$\PP( p > 0.5 \mid y) \approx 0.68  $& $\PP( p > 0.5 \mid y) =  0.81 $  \\
		$\E(p) = \int_0^1 p \cdot 1 \dx{p} = 0.5 $ & $ \E(p) = \int_0^1 p \cdot 2p \dx{p}  \approx 0.66 $  \\
		$\E(p \mid y) = \int_0^1 12 \cdot p^3 \cdot (1-p) \dx{p} = 0.6$ & $\E(p \mid y) = \int_0^1 20  \cdot p^4 \cdot (1-p) \dx{p} \approx 0.66$ \\
		\bottomrule
	\end{tabularx}
\end{table}

Видим, что в первой ситуации вероятность того, что карасей больше чем щук, при учёте наблюдений увеличивается. Ровно как и доля карасей. Во втором случае, грубо говоря,  мои наблюдения подтверждают мнение бабушки и математическое ожидание не изменяется. По той же причине вероятность того, что карасей больше чем щук увеличивается ещё сильнее.

Кстати говоря, иногда возникают ситуации, в которых апостериорный результат не зависит от того, во что мы верим. Это говорит о том, что у нас очень много данных и взаимосвязь в них прослеживается достаточно чётко.

\indef{Ещё раз, ещё раз:}

\begin{itemize}
	\item  априорное распределение выбирается до сбора данных;
	\item  с помощью априорного распределения мы пытаемся описать своё незнание;
	\item  оно отбрасывает заведомо неверные значения параметра;
	\item  вы должны быть готовы сделать денежную ставку на выбранное вами априорное распределение;
	\item  на выходе мы получаем целое апостериорное распределение, с помощью которого можем отвечать на разные вопросы.
\end{itemize}


\section*{О точечных оценках}

Мы получаем на выходе гораздо больше, чем просто точечную оценку. В конечном итоге вся информация о параметре $p$ содержится в его апостериорном распределении, с помощью которого можно отвечать на любые вопросы, касающиеся этого параметра.

Тем не менее, если от нас требуют точечную оценку, в качестве неё мы могли бы использовать, математическое ожидание, моду или медиану. Конкретный выбор зависит от того как именно нас накажу за то, если мы ошибёмся. Выбор точечной $\beta_F$ зависит от выбранной функции потерь. 

Так, например, если мы угадали параметр, то в награду получаем половину царства и принцессу, а если не угадали, то нам отрубают голову, выгоднее всего для нас назвать самое вероятное значение параметра, то есть моду апостериорного распределения.

Если функция потерь квадратичная, $(\beta_F - \beta)^2$, у нас отнимают площадь царства (и, возможно, площадь принцессы) пропорциональную квадрату отклонения спрогнозированного нами значения от настоящего, то выгоднее всего назвать в качестве оценки математическое ожидание апостериорного распределения.

Если функция потерь абсолютная, $|\beta_F - \beta|$, то в качестве оценки выгодна медиана. Выбор функции потерь, в свою очередь, зависит от поставленной перед нами задачи.

Сконцентрируемся. Закроем глаза и попытаемся отыскать в чертогах разума определения медианы и моды.  Медиана --- это квантиль уровня $0.5$. Иными словами это такое значение случайной величины, что

\[\PP(p < \Med) = \PP(p > \Med) = 0.5.\]

Найдём её!

\[ \PP(p > \Med) = 0.5 \quad \Rightarrow \quad \int_0^{\Med} 12 \cdot p^2 \cdot (1-p) \dx{p} = 0.5  \]

Взятие этого интеграла приведёт нас к уравнению четвёртой степени. Нам подойдёт решение $\Med(p) \approx 0.61 $. Скорее всего, слова <<уравнение четвёртой степени>> оставили у впечатлительного  читателя не очень хороший осадок. Компьютеры умеют избегать таких сложностей.

Модой непрерывной случайной величины называется такое её значение, при котором плотность распределения достигает локального максимума. Вполне логично, что $\Mod(p) = \frac{2}{3}$:

\[ (12 \cdot p^2 \cdot (1-p) )' = p \cdot (2 - 3 \cdot p) = 0 \quad \Rightarrow \quad p = \frac{2}{3} \vee p = 0.\]

Таким образом мы получили целых три точечные оценки: $0.6, 0.61$ и $0.66$. Как это не странно, они расположены довольно близко друг к другу. По мере увеличения количества наблюдений, пик апостериорного распределения будет становится всё острее, а точечные оценки будут становиться всё ближе.

Стоит отметить, что иногда в качестве точечной оценки выбирают какой-то квантиль апостериорного распределения. Когда боятся завысить прогноз, берут квантиль меньше медианы. Например, можно взять $30\%$ квантиль. Когда боятся занизить прогноз, берут кввантиль выше медианы. Например, можно взять $70\%$ квантиль. В такой ситуации мы имеем дело с квантильной функцией потерь, которая по-разному штрафует перепрогноз и недопрогноз. 


\section*{О доверительных и байесовских интервалах}

В частотном подходе мы часто делали интервальные оценки, строили доверительные (confidence) интервалы. В байесовском подходе также можно делать интервальные оценки, а именно, строить \indef{байесовские (bayesian или credible) интервалы.}

Между доверительным и байесовским интервалом есть тонкая разница. Если мы построили $95\%$ доверительный интервал, то говорить, что истинное значение параметра $p$ попадает в этот интервал с вероятностью $0.95$ неправильно. В случае доверительного интервала случайными величинами являются его границы.

Правильно сказать, что интервал накрывает истинное значение параметра с вероятностью $95\%$, и он может как содержать его, так и не содержать, но метод построения обеспечивает вероятность накрытия в $95\%.$ Это связано с тем, что мы работаем при построении интервала не с истинным значением параметра $p$, а с его оценкой $\hat p$. Для байесовского интервала, действительно, вероятность попадания параметра $p$ в него равна $0.95$. В случае байесовского подхода мы строим предиктивный интервал. 

Обычно, нам хотелось получить самые короткие интервалы. Почему самые короткие? Если Петя говорит, что с вероятностью $0.95$ температура завтра будет лежать в интервале от $2$ до $5$ градусов, а Вася говорит, что от $3$ до $10$ градусов, ошибаться они будут одинаково, в $5\%$ случаев, однако точность прогноза будет выше у Пети. Самый короткий байесовский интервал называется \indef{HPD (highest probability density interval)}. Конечно же, можно строить интервалы для любых вероятностей, а не только для $0.95$.

Для нашего случая, чтобы найти HPD, необходимо решить следующую задачу:

\begin{equation*}
\left \{
\begin{aligned}
&b - a \longrightarrow \min_{a,b} \\
&\int_a^b 12\cdot p^2 \cdot (1-p) \dx{p} = 0.95. \\
\end{aligned}
\right.
\end{equation*}

Можно взять интеграл, получить ограничение $4b^3 - 3b^4 -4a^3 + 3a^4 = 0.95$, не забыть, что $0 \le a,b \le 1$, выписать лагранджиан и получить, что $a \approx 0.23 $, $b \approx0.96 $. При этом, значение плотности апостериорного распределения в точке $a$ совпадёт для нашего случая с её значением в точке $b$.  Если у непрерывной случайной величины одна мода, тогда для самого короткого интервала $f(a) = f(b)$. Можно воспользоваться жтим и найти доверительный интервал для нашей задачи

\begin{equation*}
\left \{
\begin{aligned}
&b - a \longrightarrow \min_{a,b} \\
&f(a) = f(b).  \\
\end{aligned}
\right.
\end{equation*}

Те читатели, которые не выпали из повествования после слов <<уравнение четвёртой степени>>, сейчас, скорее всего, тоже потеряли интерес к чтению. Однако спешу обрадовать, на практике все вычислительные сложности на себя берёт компьютер, и здесь все эти примеры находятся лишь для того, чтобы показать, где именно возникают вычислительные сложности.

\subsection*{О прогнозах}

Когда мы строим какую-то модель, мы хотим на выходе получить прогноз. В данном случае нам было бы безумно интересно получить ответ на вопрос, какая рыба будет выловлена в озере следующей. Логично, что если у нас есть апостериорное распределение параметра $p$, то прогнозом будет какое-то распределение для нового значения $y$. Наш прогноз не будет точечным. Дело осталось за малым, преобразовать $f(p \mid y)$ в $\PP(y_4 = \text{карась} \mid y)$. Сделаем это несколькими способами.

\indef{Способ первый, безынтегральный:} мы знаем, что повторное математическое ожидание убирает условие, то есть

\[ \E(Z) = \E( \E(Z \mid W)). \]

Если случайная величина $Z$ принимает значения $0$ и $1$, тогда

\[  \PP(Z = 1) = \E(Z) = \E( \PP( Z = 1 \mid W)). \]

Более того, если есть какое-то дополнительное условие $A$, тогда выполнится

\[ \PP(Z=1 \mid A) = \E(\PP(Z = 1 \mid W,A) \mid A).\]

Чтобы осознать это, будем индексом под математическим ожиданием указывать относительно какого распределения мы ищем это математическое ожидание. В первой ситуации мы искали математическое ожидание относительно $\PP$, значит

\[ \E_{\PP}(Z) = \E_{\PP}(\E_{\PP}(Z|W)).\]

Если мы рассмотрим $\PP(Z=1 \mid A)$, то мы, сказав что наступило событие $A$, наложим на изначальное пространство элементарных исходов какое-то ограничение и перейдём к новой вероятностной мере $\PP_A$, для которой также выполняется

\[ \E_{\PP_A}(Z) = \E_{\PP_A}(\E_{\PP_A}(Z \mid W)) \].

Но что такое $\PP_A$? Это ничто иное, как условная вероятность некоторого события, $\PP(\mbox{ } \ldots \mid A)$. Делаем везде замену и получаем, что

\[\E_{\PP}(Z \mid A) = \E_{\PP}(\E_{\PP}(Z \mid W, A)\mid A).\]

Вернёмся к задаче и применим к ней этот факт:

\begin{multline*}
\PP(y_4 = \text{карась} \mid y_1,y_2,y_3) = \\ =  \E( \PP( y_4 = \text{карась} | p,y_1,y_2,y_3) \mid y_1,y_2,y_3) = \\ =  \E(p \mid y_1,y_2,y_3) = 0.6.
\end{multline*}

Такой хитрый способ найти прогноз не является универсальным. Поэтому посмотрим на интегралы, которые помогают сделать это в общем случае.

\indef{Способ второй, хитро-интегральный:} распишем искомую вероятность по формуле условной вероятности.

\begin{multline*}
\PP(y_4 = \text{карась} \mid y_1,y_2,y_3) = \\ =  \frac{\PP(y_1 = \text{карась},y_2 = \text{щука},y_3 = \text{карась},y_4 = \text{карась})}{\PP(y_1 = \text{карась},y_2 = \text{щука},y_3 = \text{карась})} = ^*
\end{multline*}

Найти ни верхнюю вероятность ни нижнюю в силу того, что $y_1, y_2, y_3,y_4$ и $p$ являются случайными величинами, мы не можем. Более того, эти случайные величины зависимы. Случайная величина $p$ влияет на реализацию каждой из этих трёх случайных величин. 

Заметим, что $y_1|p$,  $y_2|p$, $y_3|p$, $y_4|p$ независимые случайные величины, а $\PP(y_1 = \text{карась}) = \E(y_1 = \text{карась} \mid p)$. Воспользуемся этим:

\begin{multline*}
^* =  \frac{ \E(\PP(y_1 = 1,y_2 = 0,y_3 = 1,y_4=1 \mid p))}{\E(\PP(y_1 = 1,y_2 = 0,y_3=1 \mid p))} =  \frac{\E(p^3(1-p))}{\E(p^2(1-p))} = \\ =  \frac{\E(p^3) - \E(p^4)}{\E(p^2) - \E(p^3)} =  \frac{\frac{1}{4} - \frac{1}{5}}{\frac{1}{3} - \frac{1}{4}} = \frac{12}{20} = 0.6.
\end{multline*}

Конечно же для поиска всех математических ожиданий вида $\E(p^k)$ пришлось брать интегралы.

\indef{Способ третий, интегрально-влобовый:} поговорим о прогнозировании чуть более подробно, в общем, непрерывном случае. Из совместной плотности распределения $f(x,y)$ можно получить частную плотность $f(x)$, выинтригрировав совместную плотность по переменной $y$, а именно

\[ f(x) = \int f(x,y) \dx{y} = \int f(x \mid y) f(x) \dx{y}.\]

Эта формула является аналогом формулы для поиска полной вероятности. Мы перебираем континуальное количество гипотез для переменной $Y$ и находим плотность для $X$. Будем рассуждать для общего случая. Пусть у нас есть объясняемая переменная $y$ и объясняющая $x$. Что мы сделали? Мы сделали байесовский вывод и получили апостериорную плотность для параметра,

\[ f(\b \mid x, y) \propto f(y \mid x, \b) \cdot f(\b \mid x).\]

Теперь мы хотим перейти от известной нам апостериорной плотности для параметра $\b$ к плотности для нового значения $y_{new}$, $f(y_{new} \mid x_{new},x,y)$. Выинтегрируем из уже известных нам плотностей лишние части и получим требуемое

\begin{multline*}
f(y_{new} \mid x_{new},x, y) =\int f(y_{new}, \b \mid x_{new}, x,y) \dx{\b} = \\ =  \int f(y_{new} \mid x_{new}, x,y,\b) f(\b \mid x,y) \dx{\b}.
\end{multline*}

Под знаком интеграла находится произвведение нашей модели и апостериорной плотности распределения. Их обе мы знаем. Для случая карасей и щук получаем

\[
f(y \mid p) = \int f(y \mid p) f(p \mid y) \dx{p} = \int p \cdot f(p \mid y) \dx{p} = \E(p \mid y) = 0.6
\]

Таким образом, получаем требуемое распределение. Вероятность того, что выловлен карась, равна $0.6$. Делая всё это, мы снова сталкиваемся с вычислительными сложностями. Нужно брать интегралы. Повсюду куча интегралов. Решая упражнение с распределением Бернулли и тремя наблюдениями, мы уже накопили кучу вычислительных проблем. Позже мы немного поговорим о том, как компьютер побеждает эти пробелмы с помощью сэмплирования.
\end{document}


